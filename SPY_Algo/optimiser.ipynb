{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00df281e-bbbd-4673-844b-5fe74fa15842",
   "metadata": {},
   "source": [
    "Parameter Sensitivity Analysis (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cab20-389b-4882-8fc1-1897c20c0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running parameter grid search...\n",
      "RSI: 20, ATR: 0.5 -> Sharpe: -0.58\n",
      "RSI: 20, ATR: 1.0 -> Sharpe: -0.49\n",
      "RSI: 20, ATR: 1.5 -> Sharpe: -0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 20, ATR: 2.0 -> Sharpe: -0.33\n",
      "RSI: 25, ATR: 0.5 -> Sharpe: -0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 25, ATR: 1.0 -> Sharpe: -0.57\n",
      "RSI: 25, ATR: 1.5 -> Sharpe: -0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 25, ATR: 2.0 -> Sharpe: -0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 30, ATR: 0.5 -> Sharpe: -0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 30, ATR: 1.0 -> Sharpe: -0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 30, ATR: 1.5 -> Sharpe: -0.32\n",
      "RSI: 30, ATR: 2.0 -> Sharpe: -0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 35, ATR: 0.5 -> Sharpe: -0.64\n",
      "RSI: 35, ATR: 1.0 -> Sharpe: -0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI: 35, ATR: 1.5 -> Sharpe: -0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21968\\2910427695.py:105: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  signals['signal'].iloc[i] = position\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Engineers all necessary features from the raw OHLCV data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Relative Strength Index (RSI) - Fixed to handle division by zero\n",
    "    delta = df['spy_close'].diff()  # Use spy_close instead of 'Close'\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    \n",
    "    # Handle division by zero and NaN values\n",
    "    rs = np.where((loss == 0) | (loss.isna()), np.nan, gain / loss)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 2. Bollinger Bands\n",
    "    df['sma_20'] = df['spy_close'].rolling(window=20).mean()\n",
    "    df['std_20'] = df['spy_close'].rolling(window=20).std()\n",
    "    df['bollinger_upper'] = df['sma_20'] + (df['std_20'] * 2)\n",
    "    df['bollinger_lower'] = df['sma_20'] - (df['std_20'] * 2)\n",
    "\n",
    "    # 3. Rolling Z-Score\n",
    "    df['z_score_20'] = (df['spy_close'] - df['sma_20']) / df['std_20']\n",
    "\n",
    "    # 4. Average True Range (ATR) - Fixed column references\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['spy_close'].shift())  # Use spy_close\n",
    "    low_close = np.abs(df['Low'] - df['spy_close'].shift())    # Use spy_close\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr'] = tr.rolling(window=14).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_market_data(start_date=\"2005-01-01\"):\n",
    "    \"\"\"Downloads SPY and VIX data and engineers features.\"\"\"\n",
    "    spy_data = yf.download('SPY', start=start_date, auto_adjust=True)\n",
    "    vix_data = yf.download('^VIX', start=start_date, auto_adjust=True)\n",
    "\n",
    "    # Reset any potential multi-level indexes and ensure clean column structure\n",
    "    if isinstance(spy_data.columns, pd.MultiIndex):\n",
    "        spy_data.columns = spy_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    if isinstance(vix_data.columns, pd.MultiIndex):\n",
    "        vix_data.columns = vix_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    \n",
    "    # Select only the columns we need\n",
    "    spy_data = spy_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    vix_data = vix_data[['Close']].copy()\n",
    "    vix_data = vix_data.rename(columns={'Close': 'vix'})\n",
    "    \n",
    "    # Align the date indices properly\n",
    "    common_dates = spy_data.index.intersection(vix_data.index)\n",
    "    spy_data = spy_data.loc[common_dates]\n",
    "    vix_data = vix_data.loc[common_dates]\n",
    "    \n",
    "    # Combine into a single DataFrame using merge\n",
    "    df = spy_data.copy()\n",
    "    df['vix'] = vix_data['vix']\n",
    "    df.rename(columns={'Close': 'spy_close'}, inplace=True)  # Rename for clarity\n",
    "\n",
    "    # Calculate features using the function\n",
    "    df = calculate_features(df)\n",
    "    \n",
    "    # Calculate daily returns for the backtest\n",
    "    df['daily_return'] = df['spy_close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def generate_vol_norm_rsi_signals(df, rsi_entry=30, rsi_exit=50, atr_multiplier=1.0, vix_threshold=40):\n",
    "    \"\"\"\n",
    "    Generates trading signals based on the Volatility-Normalised RSI strategy.\n",
    "    Returns a DataFrame with a 'signal' column (1 for long, -1 for short, 0 for flat).\n",
    "    \"\"\"\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    signals['signal'] = 0\n",
    "\n",
    "    # Entry Conditions\n",
    "    long_entry_condition = (df['rsi'] < rsi_entry) & \\\n",
    "                           (df['spy_close'] < (df['spy_close'].shift(1) - (atr_multiplier * df['atr'])))\n",
    "    \n",
    "    short_entry_condition = (df['rsi'] > (100 - rsi_entry)) & \\\n",
    "                            (df['spy_close'] > (df['spy_close'].shift(1) + (atr_multiplier * df['atr'])))\n",
    "\n",
    "    # Exit Conditions\n",
    "    long_exit_condition = df['rsi'] > rsi_exit\n",
    "    short_exit_condition = df['rsi'] < (100 - rsi_exit)\n",
    "\n",
    "    # State machine to hold positions\n",
    "    position = 0\n",
    "    for i in range(len(df)):\n",
    "        if position == 0: # If flat\n",
    "            if long_entry_condition.iloc[i]:\n",
    "                position = 1\n",
    "            elif short_entry_condition.iloc[i]:\n",
    "                position = -1\n",
    "        elif position == 1: # If long\n",
    "            if long_exit_condition.iloc[i]:\n",
    "                position = 0\n",
    "        elif position == -1: # If short\n",
    "            if short_exit_condition.iloc[i]:\n",
    "                position = 0\n",
    "        signals['signal'].iloc[i] = position\n",
    "\n",
    "    # VIX Regime Filter - Fixed the assignment\n",
    "    vix_high_mask = df['vix'] > vix_threshold\n",
    "    signals.loc[vix_high_mask, 'signal'] = 0\n",
    "    \n",
    "    return signals['signal']\n",
    "\n",
    "def run_vectorised_backtest(data, signals, spread=0.0001, commission=0.0035, slippage_pct=0.0001):\n",
    "    \"\"\"\n",
    "    Runs a vectorised backtest with realistic transaction costs.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with 'daily_return' and other market data.\n",
    "        signals (pd.Series): Series with trading signals (1, -1, 0).\n",
    "        spread (float): Bid-ask spread as a fraction (e.g., 0.01% = 0.0001).\n",
    "        commission (float): Per-trade commission as a percentage of position value.\n",
    "        slippage_pct (float): Slippage as a percentage of position value.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Daily returns of the strategy.\n",
    "    \"\"\"\n",
    "    # Lag signals to prevent lookahead bias (trade on next day's open)\n",
    "    positions = signals.shift(1).fillna(0)\n",
    "    \n",
    "    # Calculate strategy returns without costs\n",
    "    strategy_returns = positions * data['daily_return']\n",
    "    \n",
    "    # Identify trade days (when position changes)\n",
    "    trades = positions.diff().abs()\n",
    "    \n",
    "    # Calculate transaction costs\n",
    "    # 1. Spread cost (incurred on every trade) - simplified as half the spread per trade\n",
    "    spread_cost = trades * (spread / 2)\n",
    "    \n",
    "    # 2. Commission cost (as a percentage of the trade value)\n",
    "    commission_cost = trades * commission\n",
    "    \n",
    "    # 3. Slippage cost (as a percentage of the trade value)\n",
    "    slippage_cost = trades * slippage_pct\n",
    "    \n",
    "    total_costs = spread_cost + commission_cost + slippage_cost\n",
    "    \n",
    "    # Subtract costs from returns on trade days\n",
    "    # Only apply costs when trades actually occur\n",
    "    strategy_returns = strategy_returns - total_costs\n",
    "    \n",
    "    return strategy_returns.fillna(0)\n",
    "\n",
    "def calculate_sharpe_ratio(returns):\n",
    "    \"\"\"Calculate Sharpe ratio.\"\"\"\n",
    "    if returns.std() == 0:\n",
    "        return np.nan\n",
    "    return returns.mean() / returns.std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "def display_performance(strategy_returns):\n",
    "    \"\"\"Prints a performance summary.\"\"\"\n",
    "    print(\"--- Strategy Performance ---\")\n",
    "    print(f\"Total Return: {(strategy_returns + 1).prod() - 1:.4f}\")\n",
    "    print(f\"Annual Return: {strategy_returns.mean() * 252:.4f}\")\n",
    "    print(f\"Annual Volatility: {strategy_returns.std() * np.sqrt(252):.4f}\")\n",
    "    print(f\"Sharpe Ratio: {calculate_sharpe_ratio(strategy_returns):.4f}\")\n",
    "    print(f\"Max Drawdown: {calculate_max_drawdown(strategy_returns):.4f}\")\n",
    "    print(f\"Win Rate: {calculate_win_rate(strategy_returns):.4f}\")\n",
    "    \n",
    "def calculate_max_drawdown(returns):\n",
    "    \"\"\"Calculate maximum drawdown.\"\"\"\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    return drawdown.min()\n",
    "\n",
    "def calculate_win_rate(returns):\n",
    "    \"\"\"Calculate win rate.\"\"\"\n",
    "    non_zero_returns = returns[returns != 0]\n",
    "    if len(non_zero_returns) == 0:\n",
    "        return 0.0\n",
    "    return (non_zero_returns > 0).sum() / len(non_zero_returns)\n",
    "\n",
    "# 1. Load data and define in-sample period\n",
    "full_data = get_market_data()\n",
    "in_sample_data = full_data.loc['2005-01-01':'2014-12-31']\n",
    "\n",
    "# 2. Define parameter grid\n",
    "rsi_thresholds = [20, 25, 30, 35]\n",
    "atr_multipliers = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 3. Run grid search\n",
    "print(\"Running parameter grid search...\")\n",
    "for rsi in rsi_thresholds:\n",
    "    for atr in atr_multipliers:\n",
    "        signals = generate_vol_norm_rsi_signals(\n",
    "            in_sample_data, \n",
    "            rsi_entry=rsi, \n",
    "            atr_multiplier=atr\n",
    "        )\n",
    "        \n",
    "        returns = run_vectorised_backtest(in_sample_data, signals)\n",
    "        sharpe = calculate_sharpe_ratio(returns)\n",
    "        \n",
    "        results.append({\n",
    "            'rsi_threshold': rsi,\n",
    "            'atr_multiplier': atr,\n",
    "            'sharpe_ratio': sharpe\n",
    "        })\n",
    "        print(f\"RSI: {rsi}, ATR: {atr} -> Sharpe: {sharpe:.2f}\")\n",
    "\n",
    "# 4. Process and visualize results\n",
    "results_df = pd.DataFrame(results)\n",
    "heatmap_data = results_df.pivot(\n",
    "    index='atr_multiplier', \n",
    "    columns='rsi_threshold', \n",
    "    values='sharpe_ratio'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'Sharpe Ratio'})\n",
    "plt.title(\"Parameter Sensitivity Heatmap: In-Sample Sharpe Ratio\")\n",
    "plt.xlabel(\"RSI Entry Threshold\")\n",
    "plt.ylabel(\"ATR Multiplier\")\n",
    "plt.show()\n",
    "\n",
    "# Identify the best parameters from the \"plateau\"\n",
    "# Find the best combination\n",
    "best_idx = results_df['sharpe_ratio'].idxmax()\n",
    "best_params = {\n",
    "    'rsi_entry': results_df.loc[best_idx, 'rsi_threshold'],\n",
    "    'atr_multiplier': results_df.loc[best_idx, 'atr_multiplier']\n",
    "}\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best Sharpe ratio: {results_df.loc[best_idx, 'sharpe_ratio']:.4f}\")\n",
    "\n",
    "# Define out-of-sample period\n",
    "out_of_sample_data = full_data.loc['2015-01-01':]\n",
    "\n",
    "# Generate signals and run backtest on OOS data with best parameters\n",
    "oos_signals = generate_vol_norm_rsi_signals(out_of_sample_data, **best_params)\n",
    "oos_returns = run_vectorised_backtest(out_of_sample_data, oos_signals)\n",
    "\n",
    "print(\"\\n--- Out-of-Sample Performance ---\")\n",
    "display_performance(oos_returns)\n",
    "\n",
    "# Plot out-of-sample performance\n",
    "plt.figure(figsize=(15, 7))\n",
    "cumulative_returns = (1 + oos_returns).cumprod()\n",
    "plt.plot(cumulative_returns.index, cumulative_returns, label='Out-of-Sample Strategy Returns')\n",
    "plt.title('Out-of-Sample Strategy Cumulative Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc75d51-2d66-4fdc-9ea2-574ebbcbeb63",
   "metadata": {},
   "source": [
    "# Price Target Exit (long only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcc084-1943-45e7-8572-7cf283bba940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Engineers all necessary features from the raw OHLCV data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Relative Strength Index (RSI) - Fixed to handle division by zero\n",
    "    delta = df['spy_close'].diff()  # Use spy_close instead of 'Close'\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    \n",
    "    # Handle division by zero and NaN values\n",
    "    rs = np.where((loss == 0) | (loss.isna()), np.nan, gain / loss)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 2. Bollinger Bands\n",
    "    df['sma_20'] = df['spy_close'].rolling(window=20).mean()\n",
    "    df['std_20'] = df['spy_close'].rolling(window=20).std()\n",
    "    df['bollinger_upper'] = df['sma_20'] + (df['std_20'] * 2)\n",
    "    df['bollinger_lower'] = df['sma_20'] - (df['std_20'] * 2)\n",
    "\n",
    "    # 3. Rolling Z-Score\n",
    "    df['z_score_20'] = (df['spy_close'] - df['sma_20']) / df['std_20']\n",
    "\n",
    "    # 4. Average True Range (ATR) - Fixed column references\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['spy_close'].shift())  # Use spy_close\n",
    "    low_close = np.abs(df['Low'] - df['spy_close'].shift())    # Use spy_close\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr'] = tr.rolling(window=14).mean()\n",
    "\n",
    "    # 5. Add Volume Moving Average\n",
    "    df['volume_sma_50'] = df['Volume'].rolling(window=50).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_market_data(start_date=\"2005-01-01\"):\n",
    "    \"\"\"Downloads SPY and VIX data and engineers features.\"\"\"\n",
    "    spy_data = yf.download('SPY', start=start_date, auto_adjust=True)\n",
    "    vix_data = yf.download('^VIX', start=start_date, auto_adjust=True)\n",
    "\n",
    "    # Reset any potential multi-level indexes and ensure clean column structure\n",
    "    if isinstance(spy_data.columns, pd.MultiIndex):\n",
    "        spy_data.columns = spy_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    if isinstance(vix_data.columns, pd.MultiIndex):\n",
    "        vix_data.columns = vix_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    \n",
    "    # Select only the columns we need\n",
    "    spy_data = spy_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    vix_data = vix_data[['Close']].copy()\n",
    "    vix_data = vix_data.rename(columns={'Close': 'vix'})\n",
    "    \n",
    "    # Align the date indices properly\n",
    "    common_dates = spy_data.index.intersection(vix_data.index)\n",
    "    spy_data = spy_data.loc[common_dates]\n",
    "    vix_data = vix_data.loc[common_dates]\n",
    "    \n",
    "    # Combine into a single DataFrame using merge\n",
    "    df = spy_data.copy()\n",
    "    df['vix'] = vix_data['vix']\n",
    "    df.rename(columns={'Close': 'spy_close'}, inplace=True)  # Rename for clarity\n",
    "\n",
    "    # Calculate features using the function\n",
    "    df = calculate_features(df)\n",
    "    \n",
    "    # Calculate daily returns for the backtest\n",
    "    df['daily_return'] = df['spy_close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def generate_sma_exit_signals(df, rsi_entry=30, atr_multiplier=1.5, vix_threshold=40):\n",
    "    \"\"\"\n",
    "    Generates signals with an exit condition based on crossing the 20-day SMA.\n",
    "    \"\"\"\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    signals['signal'] = 0.0\n",
    "    position = 0  # 0: flat, 1: long, -1: short\n",
    "\n",
    "    # --- ENTRY CONDITIONS (Modified) ---\n",
    "    long_entry = (df['rsi'] < rsi_entry) & \\\n",
    "                 (df['spy_close'] < (df['spy_close'].shift(1) - (atr_multiplier * df['atr']))) & \\\n",
    "                 (df['spy_close'] < df['bollinger_lower'])  # <-- ADD THIS LINE\n",
    "    \n",
    "    short_entry = (df['rsi'] > (100 - rsi_entry)) & \\\n",
    "                  (df['spy_close'] > (df['spy_close'].shift(1) + (atr_multiplier * df['atr']))) & \\\n",
    "                  (df['spy_close'] > df['bollinger_upper']) # <-- ADD THIS LINE\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        # --- EXIT CONDITIONS ---\n",
    "        # Exit long if price crosses above the 20-day SMA\n",
    "        if position == 1 and (df['spy_close'].iloc[i] > df['sma_20'].iloc[i]):\n",
    "            position = 0\n",
    "        # Exit short if price crosses below the 20-day SMA\n",
    "        elif position == -1 and (df['spy_close'].iloc[i] < df['sma_20'].iloc[i]):\n",
    "            position = 0\n",
    "        \n",
    "        # --- ENTRY LOGIC ---\n",
    "        if position == 0:\n",
    "            if long_entry.iloc[i]:\n",
    "                position = 1\n",
    "            elif short_entry.iloc[i]:\n",
    "                position = 0 # stop shorting\n",
    "        \n",
    "        signals['signal'].iloc[i] = position\n",
    "\n",
    "    # VIX Regime Filter\n",
    "    signals.loc[df['vix'] > vix_threshold, 'signal'] = 0\n",
    "    \n",
    "    return signals['signal']\n",
    "\n",
    "def run_vectorised_backtest(data, signals, spread=0.0001, commission=0.0035, slippage_pct=0.0001):\n",
    "    \"\"\"\n",
    "    Runs a vectorised backtest with realistic transaction costs.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with 'daily_return' and other market data.\n",
    "        signals (pd.Series): Series with trading signals (1, -1, 0).\n",
    "        spread (float): Bid-ask spread as a fraction (e.g., 0.01% = 0.0001).\n",
    "        commission (float): Per-trade commission as a percentage of position value.\n",
    "        slippage_pct (float): Slippage as a percentage of position value.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Daily returns of the strategy.\n",
    "    \"\"\"\n",
    "    # Lag signals to prevent lookahead bias (trade on next day's open)\n",
    "    positions = signals.shift(1).fillna(0)\n",
    "    \n",
    "    # Calculate strategy returns without costs\n",
    "    strategy_returns = positions * data['daily_return']\n",
    "    \n",
    "    # Identify trade days (when position changes)\n",
    "    trades = positions.diff().abs()\n",
    "    \n",
    "    # Calculate transaction costs\n",
    "    # 1. Spread cost (incurred on every trade) - simplified as half the spread per trade\n",
    "    spread_cost = trades * (spread / 2)\n",
    "    \n",
    "    # 2. Commission cost (as a percentage of the trade value)\n",
    "    commission_cost = trades * commission\n",
    "    \n",
    "    # 3. Slippage cost (as a percentage of the trade value)\n",
    "    slippage_cost = trades * slippage_pct\n",
    "    \n",
    "    total_costs = spread_cost + commission_cost + slippage_cost\n",
    "    \n",
    "    # Subtract costs from returns on trade days\n",
    "    # Only apply costs when trades actually occur\n",
    "    strategy_returns = strategy_returns - total_costs\n",
    "    \n",
    "    return strategy_returns.fillna(0)\n",
    "\n",
    "def calculate_sharpe_ratio(returns):\n",
    "    \"\"\"Calculate Sharpe ratio.\"\"\"\n",
    "    if returns.std() == 0:\n",
    "        return np.nan\n",
    "    return returns.mean() / returns.std() * np.sqrt(252)  # Annualised\n",
    "\n",
    "def display_performance(strategy_returns):\n",
    "    \"\"\"Prints a performance summary.\"\"\"\n",
    "    print(\"--- Strategy Performance ---\")\n",
    "    print(f\"Total Return: {(strategy_returns + 1).prod() - 1:.4f}\")\n",
    "    print(f\"Annual Return: {strategy_returns.mean() * 252:.4f}\")\n",
    "    print(f\"Annual Volatility: {strategy_returns.std() * np.sqrt(252):.4f}\")\n",
    "    print(f\"Sharpe Ratio: {calculate_sharpe_ratio(strategy_returns):.4f}\")\n",
    "    print(f\"Max Drawdown: {calculate_max_drawdown(strategy_returns):.4f}\")\n",
    "    print(f\"Win Rate: {calculate_win_rate(strategy_returns):.4f}\")\n",
    "    \n",
    "def calculate_max_drawdown(returns):\n",
    "    \"\"\"Calculate maximum drawdown.\"\"\"\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    return drawdown.min()\n",
    "\n",
    "def calculate_win_rate(returns):\n",
    "    \"\"\"Calculate win rate.\"\"\"\n",
    "    non_zero_returns = returns[returns != 0]\n",
    "    if len(non_zero_returns) == 0:\n",
    "        return 0.0\n",
    "    return (non_zero_returns > 0).sum() / len(non_zero_returns)\n",
    "\n",
    "# 1. Load data and define in-sample period\n",
    "full_data = get_market_data()\n",
    "in_sample_data = full_data.loc['2005-01-01':'2014-12-31']\n",
    "\n",
    "# 2. Define parameter grid\n",
    "rsi_thresholds = [20, 25, 30, 35]\n",
    "atr_multipliers = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 3. Run grid search\n",
    "print(\"Running parameter grid search...\")\n",
    "for rsi in rsi_thresholds:\n",
    "    for atr in atr_multipliers:\n",
    "        signals = generate_vol_norm_rsi_signals(\n",
    "            in_sample_data, \n",
    "            rsi_entry=rsi, \n",
    "            atr_multiplier=atr\n",
    "        )\n",
    "        \n",
    "        returns = run_vectorised_backtest(in_sample_data, signals)\n",
    "        sharpe = calculate_sharpe_ratio(returns)\n",
    "        \n",
    "        results.append({\n",
    "            'rsi_threshold': rsi,\n",
    "            'atr_multiplier': atr,\n",
    "            'sharpe_ratio': sharpe\n",
    "        })\n",
    "        print(f\"RSI: {rsi}, ATR: {atr} -> Sharpe: {sharpe:.2f}\")\n",
    "\n",
    "# 4. Process and visualize results\n",
    "results_df = pd.DataFrame(results)\n",
    "heatmap_data = results_df.pivot(\n",
    "    index='atr_multiplier', \n",
    "    columns='rsi_threshold', \n",
    "    values='sharpe_ratio'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'Sharpe Ratio'})\n",
    "plt.title(\"Parameter Sensitivity Heatmap: In-Sample Sharpe Ratio\")\n",
    "plt.xlabel(\"RSI Entry Threshold\")\n",
    "plt.ylabel(\"ATR Multiplier\")\n",
    "plt.show()\n",
    "\n",
    "# Identify the best parameters from the \"plateau\"\n",
    "# Find the best combination\n",
    "best_idx = results_df['sharpe_ratio'].idxmax()\n",
    "best_params = {\n",
    "    'rsi_entry': results_df.loc[best_idx, 'rsi_threshold'],\n",
    "    'atr_multiplier': results_df.loc[best_idx, 'atr_multiplier']\n",
    "}\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best Sharpe ratio: {results_df.loc[best_idx, 'sharpe_ratio']:.4f}\")\n",
    "\n",
    "# Define out-of-sample period\n",
    "out_of_sample_data = full_data.loc['2015-01-01':]\n",
    "\n",
    "# Generate signals and run backtest on OOS data with best parameters\n",
    "oos_signals = generate_sma_exit_signals(out_of_sample_data, **best_params)\n",
    "oos_returns = run_vectorised_backtest(out_of_sample_data, oos_signals)\n",
    "\n",
    "print(\"\\n--- Out-of-Sample Performance ---\")\n",
    "display_performance(oos_returns)\n",
    "\n",
    "# Plot out-of-sample performance\n",
    "plt.figure(figsize=(15, 7))\n",
    "cumulative_returns = (1 + oos_returns).cumprod()\n",
    "plt.plot(cumulative_returns.index, cumulative_returns, label='Out-of-Sample Strategy Returns')\n",
    "plt.title('Out-of-Sample Strategy Cumulative Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331406b-9fdb-4a89-a4fb-2b7d414f2596",
   "metadata": {},
   "source": [
    "# Bollinger + Volume Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8c662-3d35-44c0-956b-89813f96b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Engineers all necessary features from the raw OHLCV data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Relative Strength Index (RSI) - Fixed to handle division by zero\n",
    "    delta = df['spy_close'].diff()  # Use spy_close instead of 'Close'\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    \n",
    "    # Handle division by zero and NaN values\n",
    "    rs = np.where((loss == 0) | (loss.isna()), np.nan, gain / loss)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 2. Bollinger Bands\n",
    "    df['sma_20'] = df['spy_close'].rolling(window=20).mean()\n",
    "    df['std_20'] = df['spy_close'].rolling(window=20).std()\n",
    "    df['bollinger_upper'] = df['sma_20'] + (df['std_20'] * 2)\n",
    "    df['bollinger_lower'] = df['sma_20'] - (df['std_20'] * 2)\n",
    "\n",
    "    # 3. Rolling Z-Score\n",
    "    df['z_score_20'] = (df['spy_close'] - df['sma_20']) / df['std_20']\n",
    "\n",
    "    # 4. Average True Range (ATR) - Fixed column references\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['spy_close'].shift())  # Use spy_close\n",
    "    low_close = np.abs(df['Low'] - df['spy_close'].shift())    # Use spy_close\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr'] = tr.rolling(window=14).mean()\n",
    "\n",
    "    # 5. Add Volume Moving Average\n",
    "    df['volume_sma_50'] = df['Volume'].rolling(window=50).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_market_data(start_date=\"2005-01-01\"):\n",
    "    \"\"\"Downloads SPY and VIX data and engineers features.\"\"\"\n",
    "    spy_data = yf.download('SPY', start=start_date, auto_adjust=True)\n",
    "    vix_data = yf.download('^VIX', start=start_date, auto_adjust=True)\n",
    "\n",
    "    # Reset any potential multi-level indexes and ensure clean column structure\n",
    "    if isinstance(spy_data.columns, pd.MultiIndex):\n",
    "        spy_data.columns = spy_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    if isinstance(vix_data.columns, pd.MultiIndex):\n",
    "        vix_data.columns = vix_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    \n",
    "    # Select only the columns we need\n",
    "    spy_data = spy_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    vix_data = vix_data[['Close']].copy()\n",
    "    vix_data = vix_data.rename(columns={'Close': 'vix'})\n",
    "    \n",
    "    # Align the date indices properly\n",
    "    common_dates = spy_data.index.intersection(vix_data.index)\n",
    "    spy_data = spy_data.loc[common_dates]\n",
    "    vix_data = vix_data.loc[common_dates]\n",
    "    \n",
    "    # Combine into a single DataFrame using merge\n",
    "    df = spy_data.copy()\n",
    "    df['vix'] = vix_data['vix']\n",
    "    df.rename(columns={'Close': 'spy_close'}, inplace=True)  # Rename for clarity\n",
    "\n",
    "    # Calculate features using the function\n",
    "    df = calculate_features(df)\n",
    "    \n",
    "    # Calculate daily returns for the backtest\n",
    "    df['daily_return'] = df['spy_close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def generate_sma_exit_signals(df, rsi_entry=30, atr_multiplier=1.5, vix_threshold=40):\n",
    "    \"\"\"\n",
    "    Generates signals with an exit condition based on crossing the 20-day SMA.\n",
    "    \"\"\"\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    signals['signal'] = 0.0\n",
    "    position = 0  # 0: flat, 1: long, -1: short\n",
    "\n",
    "    # --- ENTRY CONDITIONS (Modified with Volume) ---\n",
    "    long_entry = (df['rsi'] < rsi_entry) & \\\n",
    "                 (df['spy_close'] < df['bollinger_lower']) & \\\n",
    "                 (df['Volume'] > df['volume_sma_50'] * 1.5) \n",
    "    \n",
    "    short_entry = (df['rsi'] > (100 - rsi_entry)) & \\\n",
    "                  (df['spy_close'] > df['bollinger_upper']) & \\\n",
    "                  (df['Volume'] > df['volume_sma_50'] * 1.5)\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        # --- EXIT CONDITIONS ---\n",
    "        # Exit long if price crosses above the 20-day SMA\n",
    "        if position == 1 and (df['spy_close'].iloc[i] > df['sma_20'].iloc[i]):\n",
    "            position = 0\n",
    "        # Exit short if price crosses below the 20-day SMA\n",
    "        elif position == -1 and (df['spy_close'].iloc[i] < df['sma_20'].iloc[i]):\n",
    "            position = 0\n",
    "        \n",
    "        # --- ENTRY LOGIC ---\n",
    "        if position == 0:\n",
    "            if long_entry.iloc[i]:\n",
    "                position = 1\n",
    "            elif short_entry.iloc[i]:\n",
    "                position = 0 # stop shorting\n",
    "        \n",
    "        signals['signal'].iloc[i] = position\n",
    "\n",
    "    # VIX Regime Filter\n",
    "    signals.loc[df['vix'] > vix_threshold, 'signal'] = 0\n",
    "    \n",
    "    return signals['signal']\n",
    "\n",
    "def run_vectorised_backtest(data, signals, spread=0.0001, commission=0.0035, slippage_pct=0.0001):\n",
    "    \"\"\"\n",
    "    Runs a vectorised backtest with realistic transaction costs.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with 'daily_return' and other market data.\n",
    "        signals (pd.Series): Series with trading signals (1, -1, 0).\n",
    "        spread (float): Bid-ask spread as a fraction (e.g., 0.01% = 0.0001).\n",
    "        commission (float): Per-trade commission as a percentage of position value.\n",
    "        slippage_pct (float): Slippage as a percentage of position value.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Daily returns of the strategy.\n",
    "    \"\"\"\n",
    "    # Lag signals to prevent lookahead bias (trade on next day's open)\n",
    "    positions = signals.shift(1).fillna(0)\n",
    "    \n",
    "    # Calculate strategy returns without costs\n",
    "    strategy_returns = positions * data['daily_return']\n",
    "    \n",
    "    # Identify trade days (when position changes)\n",
    "    trades = positions.diff().abs()\n",
    "    \n",
    "    # Calculate transaction costs\n",
    "    # 1. Spread cost (incurred on every trade) - simplified as half the spread per trade\n",
    "    spread_cost = trades * (spread / 2)\n",
    "    \n",
    "    # 2. Commission cost (as a percentage of the trade value)\n",
    "    commission_cost = trades * commission\n",
    "    \n",
    "    # 3. Slippage cost (as a percentage of the trade value)\n",
    "    slippage_cost = trades * slippage_pct\n",
    "    \n",
    "    total_costs = spread_cost + commission_cost + slippage_cost\n",
    "    \n",
    "    # Subtract costs from returns on trade days\n",
    "    # Only apply costs when trades actually occur\n",
    "    strategy_returns = strategy_returns - total_costs\n",
    "    \n",
    "    return strategy_returns.fillna(0)\n",
    "\n",
    "def calculate_sharpe_ratio(returns):\n",
    "    \"\"\"Calculate Sharpe ratio.\"\"\"\n",
    "    if returns.std() == 0:\n",
    "        return np.nan\n",
    "    return returns.mean() / returns.std() * np.sqrt(252)  # Annualised\n",
    "\n",
    "def display_performance(strategy_returns):\n",
    "    \"\"\"Prints a performance summary.\"\"\"\n",
    "    print(\"--- Strategy Performance ---\")\n",
    "    print(f\"Total Return: {(strategy_returns + 1).prod() - 1:.4f}\")\n",
    "    print(f\"Annual Return: {strategy_returns.mean() * 252:.4f}\")\n",
    "    print(f\"Annual Volatility: {strategy_returns.std() * np.sqrt(252):.4f}\")\n",
    "    print(f\"Sharpe Ratio: {calculate_sharpe_ratio(strategy_returns):.4f}\")\n",
    "    print(f\"Max Drawdown: {calculate_max_drawdown(strategy_returns):.4f}\")\n",
    "    print(f\"Win Rate: {calculate_win_rate(strategy_returns):.4f}\")\n",
    "    \n",
    "def calculate_max_drawdown(returns):\n",
    "    \"\"\"Calculate maximum drawdown.\"\"\"\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    return drawdown.min()\n",
    "\n",
    "def calculate_win_rate(returns):\n",
    "    \"\"\"Calculate win rate.\"\"\"\n",
    "    non_zero_returns = returns[returns != 0]\n",
    "    if len(non_zero_returns) == 0:\n",
    "        return 0.0\n",
    "    return (non_zero_returns > 0).sum() / len(non_zero_returns)\n",
    "\n",
    "# 1. Load data and define in-sample period\n",
    "full_data = get_market_data()\n",
    "in_sample_data = full_data.loc['2005-01-01':'2014-12-31']\n",
    "\n",
    "# 2. Define parameter grid\n",
    "rsi_thresholds = [20, 25, 30, 35]\n",
    "atr_multipliers = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 3. Run grid search\n",
    "print(\"Running parameter grid search...\")\n",
    "for rsi in rsi_thresholds:\n",
    "    for atr in atr_multipliers:\n",
    "        signals = generate_vol_norm_rsi_signals(\n",
    "            in_sample_data, \n",
    "            rsi_entry=rsi, \n",
    "            atr_multiplier=atr\n",
    "        )\n",
    "        \n",
    "        returns = run_vectorised_backtest(in_sample_data, signals)\n",
    "        sharpe = calculate_sharpe_ratio(returns)\n",
    "        \n",
    "        results.append({\n",
    "            'rsi_threshold': rsi,\n",
    "            'atr_multiplier': atr,\n",
    "            'sharpe_ratio': sharpe\n",
    "        })\n",
    "        print(f\"RSI: {rsi}, ATR: {atr} -> Sharpe: {sharpe:.2f}\")\n",
    "\n",
    "# 4. Process and visualize results\n",
    "results_df = pd.DataFrame(results)\n",
    "heatmap_data = results_df.pivot(\n",
    "    index='atr_multiplier', \n",
    "    columns='rsi_threshold', \n",
    "    values='sharpe_ratio'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'Sharpe Ratio'})\n",
    "plt.title(\"Parameter Sensitivity Heatmap: In-Sample Sharpe Ratio\")\n",
    "plt.xlabel(\"RSI Entry Threshold\")\n",
    "plt.ylabel(\"ATR Multiplier\")\n",
    "plt.show()\n",
    "\n",
    "# Identify the best parameters from the \"plateau\"\n",
    "# Find the best combination\n",
    "best_idx = results_df['sharpe_ratio'].idxmax()\n",
    "best_params = {\n",
    "    'rsi_entry': results_df.loc[best_idx, 'rsi_threshold'],\n",
    "    'atr_multiplier': results_df.loc[best_idx, 'atr_multiplier']\n",
    "}\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best Sharpe ratio: {results_df.loc[best_idx, 'sharpe_ratio']:.4f}\")\n",
    "\n",
    "# Define out-of-sample period\n",
    "out_of_sample_data = full_data.loc['2015-01-01':]\n",
    "\n",
    "# Generate signals and run backtest on OOS data with best parameters\n",
    "oos_signals = generate_sma_exit_signals(out_of_sample_data, **best_params)\n",
    "oos_returns = run_vectorised_backtest(out_of_sample_data, oos_signals)\n",
    "\n",
    "print(\"\\n--- Out-of-Sample Performance ---\")\n",
    "display_performance(oos_returns)\n",
    "\n",
    "# Plot out-of-sample performance\n",
    "plt.figure(figsize=(15, 7))\n",
    "cumulative_returns = (1 + oos_returns).cumprod()\n",
    "plt.plot(cumulative_returns.index, cumulative_returns, label='Out-of-Sample Strategy Returns')\n",
    "plt.title('Out-of-Sample Strategy Cumulative Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9e827-3576-4fd9-8ef4-de26dda1f789",
   "metadata": {},
   "source": [
    "# Adjusting rsi_entry and atr_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7296e04-56c5-4249-a3f0-26612273bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Engineers all necessary features from the raw OHLCV data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Relative Strength Index (RSI) - Fixed to handle division by zero\n",
    "    delta = df['spy_close'].diff()  # Use spy_close instead of 'Close'\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    \n",
    "    # Handle division by zero and NaN values\n",
    "    rs = np.where((loss == 0) | (loss.isna()), np.nan, gain / loss)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 2. Bollinger Bands\n",
    "    df['sma_20'] = df['spy_close'].rolling(window=20).mean()\n",
    "    df['std_20'] = df['spy_close'].rolling(window=20).std()\n",
    "    df['bollinger_upper'] = df['sma_20'] + (df['std_20'] * 2)\n",
    "    df['bollinger_lower'] = df['sma_20'] - (df['std_20'] * 2)\n",
    "\n",
    "    # 3. Rolling Z-Score\n",
    "    df['z_score_20'] = (df['spy_close'] - df['sma_20']) / df['std_20']\n",
    "\n",
    "    # 4. Average True Range (ATR) - Fixed column references\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['spy_close'].shift())  # Use spy_close\n",
    "    low_close = np.abs(df['Low'] - df['spy_close'].shift())    # Use spy_close\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr'] = tr.rolling(window=14).mean()\n",
    "\n",
    "    # 5. Add Volume Moving Average\n",
    "    df['volume_sma_50'] = df['Volume'].rolling(window=50).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_market_data(start_date=\"2005-01-01\"):\n",
    "    \"\"\"Downloads SPY and VIX data and engineers features.\"\"\"\n",
    "    spy_data = yf.download('SPY', start=start_date, auto_adjust=True)\n",
    "    vix_data = yf.download('^VIX', start=start_date, auto_adjust=True)\n",
    "\n",
    "    # Reset any potential multi-level indexes and ensure clean column structure\n",
    "    if isinstance(spy_data.columns, pd.MultiIndex):\n",
    "        spy_data.columns = spy_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    if isinstance(vix_data.columns, pd.MultiIndex):\n",
    "        vix_data.columns = vix_data.columns.droplevel(1)  # Remove the second level if it exists\n",
    "    \n",
    "    # Select only the columns we need\n",
    "    spy_data = spy_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    vix_data = vix_data[['Close']].copy()\n",
    "    vix_data = vix_data.rename(columns={'Close': 'vix'})\n",
    "    \n",
    "    # Align the date indices properly\n",
    "    common_dates = spy_data.index.intersection(vix_data.index)\n",
    "    spy_data = spy_data.loc[common_dates]\n",
    "    vix_data = vix_data.loc[common_dates]\n",
    "    \n",
    "    # Combine into a single DataFrame using merge\n",
    "    df = spy_data.copy()\n",
    "    df['vix'] = vix_data['vix']\n",
    "    df.rename(columns={'Close': 'spy_close'}, inplace=True)  # Rename for clarity\n",
    "\n",
    "    # Calculate features using the function\n",
    "    df = calculate_features(df)\n",
    "    \n",
    "    # Calculate daily returns for the backtest\n",
    "    df['daily_return'] = df['spy_close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def generate_sma_exit_signals(df, rsi_entry=25, atr_multiplier=1.5, vix_threshold=40):\n",
    "\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    signals['signal'] = 0.0\n",
    "    position = 0  # 0: flat, 1: long, -1: short\n",
    "\n",
    "    # --- ENTRY CONDITIONS (Modified with Volume) ---\n",
    "    long_entry = (df['rsi'] < rsi_entry) & \\\n",
    "                 (df['spy_close'] < df['bollinger_lower']) & \\\n",
    "                 (df['Volume'] > df['volume_sma_50'] * 1.5) \n",
    "    \n",
    "    short_entry = (df['rsi'] > (100 - rsi_entry)) & \\\n",
    "                  (df['spy_close'] > df['bollinger_upper']) & \\\n",
    "                  (df['Volume'] > df['volume_sma_50'] * 1.5)\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        # --- EXIT CONDITIONS ---\n",
    "        # Exit long if price crosses above the 20-day SMA\n",
    "        if position == 1 and (df['spy_close'].iloc[i] > df['sma_20'].iloc[i]):\n",
    "            position = 0\n",
    "        # Exit short if price crosses below the 20-day SMA\n",
    "        elif position == -1 and (df['spy_close'].iloc[i] < df['sma_20'].iloc[i]):\n",
    "            position = 0\n",
    "        \n",
    "        # --- ENTRY LOGIC ---\n",
    "        if position == 0:\n",
    "            if long_entry.iloc[i]:\n",
    "                position = 1\n",
    "            elif short_entry.iloc[i]:\n",
    "                position = 0 # stop shorting\n",
    "        \n",
    "        signals['signal'].iloc[i] = position\n",
    "\n",
    "    # VIX Regime Filter\n",
    "    signals.loc[df['vix'] > vix_threshold, 'signal'] = 0\n",
    "    \n",
    "    return signals['signal']\n",
    "\n",
    "def run_vectorised_backtest(data, signals, spread=0.0001, commission=0.0035, slippage_pct=0.0001):\n",
    "    \"\"\"\n",
    "    Runs a vectorised backtest with realistic transaction costs.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with 'daily_return' and other market data.\n",
    "        signals (pd.Series): Series with trading signals (1, -1, 0).\n",
    "        spread (float): Bid-ask spread as a fraction (e.g., 0.01% = 0.0001).\n",
    "        commission (float): Per-trade commission as a percentage of position value.\n",
    "        slippage_pct (float): Slippage as a percentage of position value.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Daily returns of the strategy.\n",
    "    \"\"\"\n",
    "    # Lag signals to prevent lookahead bias (trade on next day's open)\n",
    "    positions = signals.shift(1).fillna(0)\n",
    "    \n",
    "    # Calculate strategy returns without costs\n",
    "    strategy_returns = positions * data['daily_return']\n",
    "    \n",
    "    # Identify trade days (when position changes)\n",
    "    trades = positions.diff().abs()\n",
    "    \n",
    "    # Calculate transaction costs\n",
    "    # 1. Spread cost (incurred on every trade) - simplified as half the spread per trade\n",
    "    spread_cost = trades * (spread / 2)\n",
    "    \n",
    "    # 2. Commission cost (as a percentage of the trade value)\n",
    "    commission_cost = trades * commission\n",
    "    \n",
    "    # 3. Slippage cost (as a percentage of the trade value)\n",
    "    slippage_cost = trades * slippage_pct\n",
    "    \n",
    "    total_costs = spread_cost + commission_cost + slippage_cost\n",
    "    \n",
    "    # Subtract costs from returns on trade days\n",
    "    # Only apply costs when trades actually occur\n",
    "    strategy_returns = strategy_returns - total_costs\n",
    "    \n",
    "    return strategy_returns.fillna(0)\n",
    "\n",
    "def calculate_sharpe_ratio(returns):\n",
    "    \"\"\"Calculate Sharpe ratio.\"\"\"\n",
    "    if returns.std() == 0:\n",
    "        return np.nan\n",
    "    return returns.mean() / returns.std() * np.sqrt(252)  # Annualised\n",
    "\n",
    "def display_performance(strategy_returns):\n",
    "    \"\"\"Prints a performance summary.\"\"\"\n",
    "    print(\"--- Strategy Performance ---\")\n",
    "    print(f\"Total Return: {(strategy_returns + 1).prod() - 1:.4f}\")\n",
    "    print(f\"Annual Return: {strategy_returns.mean() * 252:.4f}\")\n",
    "    print(f\"Annual Volatility: {strategy_returns.std() * np.sqrt(252):.4f}\")\n",
    "    print(f\"Sharpe Ratio: {calculate_sharpe_ratio(strategy_returns):.4f}\")\n",
    "    print(f\"Max Drawdown: {calculate_max_drawdown(strategy_returns):.4f}\")\n",
    "    print(f\"Win Rate: {calculate_win_rate(strategy_returns):.4f}\")\n",
    "    \n",
    "def calculate_max_drawdown(returns):\n",
    "    \"\"\"Calculate maximum drawdown.\"\"\"\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    return drawdown.min()\n",
    "\n",
    "def calculate_win_rate(returns):\n",
    "    \"\"\"Calculate win rate.\"\"\"\n",
    "    non_zero_returns = returns[returns != 0]\n",
    "    if len(non_zero_returns) == 0:\n",
    "        return 0.0\n",
    "    return (non_zero_returns > 0).sum() / len(non_zero_returns)\n",
    "\n",
    "# 1. Load data and define in-sample period\n",
    "full_data = get_market_data()\n",
    "in_sample_data = full_data.loc['2005-01-01':'2014-12-31']\n",
    "\n",
    "# 2. Define parameter grid\n",
    "rsi_thresholds = [20, 25, 30, 35]\n",
    "atr_multipliers = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 3. Run grid search\n",
    "print(\"Running parameter grid search...\")\n",
    "for rsi in rsi_thresholds:\n",
    "    for atr in atr_multipliers:\n",
    "        signals = generate_vol_norm_rsi_signals(\n",
    "            in_sample_data, \n",
    "            rsi_entry=rsi, \n",
    "            atr_multiplier=atr\n",
    "        )\n",
    "        \n",
    "        returns = run_vectorised_backtest(in_sample_data, signals)\n",
    "        sharpe = calculate_sharpe_ratio(returns)\n",
    "        \n",
    "        results.append({\n",
    "            'rsi_threshold': rsi,\n",
    "            'atr_multiplier': atr,\n",
    "            'sharpe_ratio': sharpe\n",
    "        })\n",
    "        print(f\"RSI: {rsi}, ATR: {atr} -> Sharpe: {sharpe:.2f}\")\n",
    "\n",
    "# 4. Process and visualise results\n",
    "results_df = pd.DataFrame(results)\n",
    "heatmap_data = results_df.pivot(\n",
    "    index='atr_multiplier', \n",
    "    columns='rsi_threshold', \n",
    "    values='sharpe_ratio'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'Sharpe Ratio'})\n",
    "plt.title(\"Parameter Sensitivity Heatmap: In-Sample Sharpe Ratio\")\n",
    "plt.xlabel(\"RSI Entry Threshold\")\n",
    "plt.ylabel(\"ATR Multiplier\")\n",
    "plt.show()\n",
    "\n",
    "# Identify the best parameters from the \"plateau\"\n",
    "# Find the best combination\n",
    "best_idx = results_df['sharpe_ratio'].idxmax()\n",
    "best_params = {\n",
    "    'rsi_entry': results_df.loc[best_idx, 'rsi_threshold'],\n",
    "    'atr_multiplier': results_df.loc[best_idx, 'atr_multiplier']\n",
    "}\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best Sharpe ratio: {results_df.loc[best_idx, 'sharpe_ratio']:.4f}\")\n",
    "\n",
    "# Define out-of-sample period\n",
    "out_of_sample_data = full_data.loc['2015-01-01':]\n",
    "\n",
    "# Generate signals and run backtest on OOS data with best parameters\n",
    "oos_signals = generate_sma_exit_signals(out_of_sample_data, **best_params)\n",
    "oos_returns = run_vectorised_backtest(out_of_sample_data, oos_signals)\n",
    "\n",
    "print(\"\\n--- Out-of-Sample Performance ---\")\n",
    "display_performance(oos_returns)\n",
    "\n",
    "# Plot out-of-sample performance\n",
    "plt.figure(figsize=(15, 7))\n",
    "cumulative_returns = (1 + oos_returns).cumprod()\n",
    "plt.plot(cumulative_returns.index, cumulative_returns, label='Out-of-Sample Strategy Returns')\n",
    "plt.title('Out-of-Sample Strategy Cumulative Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (quant_env)",
   "language": "python",
   "name": "quant_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
